{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a working demo of Pytorch. It is based on official tutorials referenced below.\n",
    "\n",
    "There are three main sections. First one reviews basics, second provides a skorch overview and third explains details of a broadcast based distributed example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize from Python list of list\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "initialize from NumPy array\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "initialize using builtin functions\n",
      "tensor([[0.3261, 0.4479, 0.4904],\n",
      "        [0.6012, 0.3733, 0.1714]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print( 'initialize from Python list of list' )\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "\n",
    "print( '\\ninitialize from NumPy array')\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_data)\n",
    "\n",
    "print( '\\ninitialize using builtin functions')\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(rand_tensor)\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(ones_tensor)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor attributes\n",
      "torch.Size([2, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print('tensor attributes')\n",
    "tensor = torch.rand(2,3)\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor indexing and slicing\n",
      "tensor([[1., 0., 1.],\n",
      "        [1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print('tensor indexing and slicing')\n",
    "tensor = torch.ones(2,3)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise product\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print('element-wise product')\n",
    "t1 = torch.ones(2,2)*2\n",
    "t2 = torch.ones(2,2)*3\n",
    "print(t1.mul(t2))\n",
    "print(t1*t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print('matrix multiplication')\n",
    "print(t1.matmul(t2))\n",
    "print(t1 @ t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Many operations are parallelized including autograd. Lets look at one of the basic operations of Neural Networks, differentiation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new tensor Q depending on a and b $$ Q = 3a^3 - b^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate partial derivative w.r.t a and b \n",
    "\n",
    "$$ \\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
    "\n",
    "$$ \\frac{\\partial Q}{\\partial b} = -2b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward function of tensor Q automatically calculates these partial derivatives (gradients) and stores them in respective tensor's .grad attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, backward function multiplies its input with the Jacobian matrix of input tensors and its default argument of torch.Tensor([1]). This is not a valid operation because Q is a two element vector. So we need to pass a two dimensional unit vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.backward(torch.Tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose( a.grad, 9*a**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose( b.grad, -2*b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/mnist.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1268, -0.0857,  0.0078,  0.0210,  0.0718,  0.0085,  0.0289,  0.1668,\n",
      "         -0.0282, -0.1295]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9574, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0151,  0.0089, -0.0009,  0.0077, -0.0061,  0.0254])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skorch Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn style NN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6955\u001b[0m       \u001b[32m0.5350\u001b[0m        \u001b[35m0.6922\u001b[0m  0.0404\n",
      "      2        \u001b[36m0.6828\u001b[0m       \u001b[32m0.5800\u001b[0m        \u001b[35m0.6846\u001b[0m  0.0450\n",
      "      3        \u001b[36m0.6773\u001b[0m       \u001b[32m0.6300\u001b[0m        \u001b[35m0.6787\u001b[0m  0.0448\n",
      "      4        \u001b[36m0.6699\u001b[0m       \u001b[32m0.6350\u001b[0m        \u001b[35m0.6750\u001b[0m  0.0433\n",
      "      5        \u001b[36m0.6575\u001b[0m       \u001b[32m0.6550\u001b[0m        \u001b[35m0.6685\u001b[0m  0.0427\n",
      "      6        \u001b[36m0.6526\u001b[0m       \u001b[32m0.6750\u001b[0m        \u001b[35m0.6641\u001b[0m  0.0418\n",
      "      7        \u001b[36m0.6371\u001b[0m       0.6650        \u001b[35m0.6577\u001b[0m  0.0439\n",
      "      8        \u001b[36m0.6251\u001b[0m       \u001b[32m0.6800\u001b[0m        \u001b[35m0.6491\u001b[0m  0.0435\n",
      "      9        0.6342       \u001b[32m0.6900\u001b[0m        \u001b[35m0.6376\u001b[0m  0.0455\n",
      "     10        \u001b[36m0.6137\u001b[0m       0.6900        \u001b[35m0.6283\u001b[0m  0.0366\n",
      "prediction performance 74%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=nn.ReLU()):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "net.fit(X, y)\n",
    "y_proba = net.predict_proba(X)\n",
    "\n",
    "predict_y = net.predict(X)\n",
    "print('prediction performance %.0f%%' % (sum(predict_y == y) / X.shape[0] * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=10 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=10 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=10, total=   0.3s\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=10, total=   0.3s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=10, total=   0.3s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=20, total=   0.4s\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=10, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.01, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=20, total=   0.4s\n",
      "[CV] module__num_units=20, lr=0.01, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.01, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=10, total=   0.3s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=10 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=10, total=   0.2s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=20, total=   0.4s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=10, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=10, lr=0.02, max_epochs=20, total=   0.4s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=20, total=   0.5s\n",
      "[CV] module__num_units=20, lr=0.02, max_epochs=20 ....................\n",
      "[CV] ..... module__num_units=20, lr=0.02, max_epochs=20, total=   0.5s\n",
      "best score: 0.688, best params: {'module__num_units': 20, 'lr': 0.02, 'max_epochs': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    8.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    'module__num_units': [10, 20],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Distributed Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast and gather\n",
    "\n",
    "Process with rank 0 broadcasts data to all others\n",
    "![title](img/broadcast.png)\n",
    "\n",
    "Process with rank 0 gathers all output from all others\n",
    "![title](img/gather.png)\n",
    "\n",
    "[demo](https://github.com/cankav/pytorcher/blob/master/run_distributed_broadcast_gather.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarrassingly parallel processing\n",
    "\n",
    "![title](img/pytorch_distributed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "* https://github.com/skorch-dev/skorch\n",
    "* https://github.com/cankav/pytorcher\n",
    "* https://github.com/RyersonU-DataScienceLab/pomdp_solvers/blob/master/dist_utils/distributed_utils.py\n",
    "* https://pytorch.org/tutorials/intermediate/dist_tuto.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
