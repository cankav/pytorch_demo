{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a working demo of Pytorch. It is based on official tutorials referenced below.\n",
    "\n",
    "There are three main sections. First one reviews basics, second provides a skorch overview and third explains details of a broadcast based distributed example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize from Python list of list\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "initialize from NumPy array\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "initialize using builtin functions\n",
      "tensor([[0.5408, 0.1525, 0.6799],\n",
      "        [0.5660, 0.9491, 0.9543]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print( 'initialize from Python list of list' )\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "\n",
    "print( '\\ninitialize from NumPy array')\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_data)\n",
    "\n",
    "print( '\\ninitialize using builtin functions')\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(rand_tensor)\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(ones_tensor)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor attributes\n",
      "torch.Size([2, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print('tensor attributes')\n",
    "tensor = torch.rand(2,3)\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor indexing and slicing\n",
      "tensor([[1., 0., 1.],\n",
      "        [1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print('tensor indexing and slicing')\n",
    "tensor = torch.ones(2,3)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise product\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print('element-wise product')\n",
    "t1 = torch.ones(2,2)*2\n",
    "t2 = torch.ones(2,2)*3\n",
    "print(t1.mul(t2))\n",
    "print(t1*t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print('matrix multiplication')\n",
    "print(t1.matmul(t2))\n",
    "print(t1 @ t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Many operations are parallelized including autograd. Lets look at one of the basic operations of Neural Networks, differentiation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new tensor Q depending on a and b $$ Q = 3a^3 - b^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate partial derivative w.r.t a and b \n",
    "\n",
    "$$ \\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
    "\n",
    "$$ \\frac{\\partial Q}{\\partial b} = -2b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward function of tensor Q automatically calculates these partial derivatives (gradients) and stores them in respective tensor's .grad attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, backward function multiplies its input with the Jacobian matrix of input tensors and its default argument of torch.Tensor([1]). This is not a valid operation because Q is a two element vector. So we need to pass a two dimensional unit vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.backward(torch.Tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose( a.grad, 9*a**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose( b.grad, -2*b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0515, -0.0502, -0.1390, -0.0416,  0.1407, -0.0035,  0.0550,  0.0544,\n",
      "          0.0698, -0.0672]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2178, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0050,  0.0222, -0.0110,  0.0027, -0.0116, -0.0051])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skorch Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn style NN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6946\u001b[0m       \u001b[32m0.5050\u001b[0m        \u001b[35m0.6852\u001b[0m  0.0550\n",
      "      2        \u001b[36m0.6816\u001b[0m       \u001b[32m0.5450\u001b[0m        \u001b[35m0.6788\u001b[0m  0.0379\n",
      "      3        \u001b[36m0.6682\u001b[0m       \u001b[32m0.5850\u001b[0m        \u001b[35m0.6723\u001b[0m  0.0393\n",
      "      4        \u001b[36m0.6606\u001b[0m       \u001b[32m0.5900\u001b[0m        \u001b[35m0.6669\u001b[0m  0.0403\n",
      "      5        \u001b[36m0.6529\u001b[0m       \u001b[32m0.5950\u001b[0m        \u001b[35m0.6573\u001b[0m  0.0414\n",
      "      6        \u001b[36m0.6400\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.6470\u001b[0m  0.0366\n",
      "      7        \u001b[36m0.6350\u001b[0m       \u001b[32m0.6200\u001b[0m        \u001b[35m0.6415\u001b[0m  0.0381\n",
      "      8        \u001b[36m0.6217\u001b[0m       \u001b[32m0.6450\u001b[0m        \u001b[35m0.6330\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6083\u001b[0m       \u001b[32m0.6650\u001b[0m        \u001b[35m0.6278\u001b[0m  0.0351\n",
      "     10        \u001b[36m0.5967\u001b[0m       \u001b[32m0.6850\u001b[0m        \u001b[35m0.6175\u001b[0m  0.0363\n",
      "prediction performance 71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=nn.ReLU()):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "net.fit(X, y)\n",
    "y_proba = net.predict_proba(X)\n",
    "\n",
    "predict_y = net.predict(X)\n",
    "print('prediction performance %.0f%%' % (sum(predict_y == y) / X.shape[0] * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=10 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=10, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=10 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=10, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=10, total=   0.3s\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=20, total=   0.3s\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=20, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.01, module__num_units=20, total=   0.3s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=10, total=   0.5s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=10, total=   0.4s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=10, total=   0.5s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=20, total=   0.4s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=20, total=   0.5s\n",
      "[CV] max_epochs=20, lr=0.01, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.01, module__num_units=20, total=   0.5s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=10, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=10, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=10, total=   0.3s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=20, total=   0.3s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=20, total=   0.2s\n",
      "[CV] max_epochs=10, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=10, lr=0.02, module__num_units=20, total=   0.3s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=10, total=   0.4s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=10, total=   0.5s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=10 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=10, total=   0.6s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=20, total=   0.6s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=20, total=   0.7s\n",
      "[CV] max_epochs=20, lr=0.02, module__num_units=20 ....................\n",
      "[CV] ..... max_epochs=20, lr=0.02, module__num_units=20, total=   0.4s\n",
      "best score: 0.665, best params: {'max_epochs': 20, 'lr': 0.02, 'module__num_units': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    9.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# deactivate skorch-internal train-valid split and verbose logging\n",
    "net.set_params(train_split=False, verbose=0)\n",
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    'module__num_units': [10, 20],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Distributed Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast and gather\n",
    "\n",
    "Process with rank 0 broadcasts data to all others\n",
    "![title](img/broadcast.png)\n",
    "\n",
    "Process with rank 0 gathers all output from all others\n",
    "![title](img/gather.png)\n",
    "\n",
    "[demo](https://github.com/cankav/pytorcher/blob/master/run_distributed_broadcast_gather.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarrassingly parallel processing\n",
    "\n",
    "![title](img/pytorch_distributed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "* https://github.com/skorch-dev/skorch\n",
    "* https://github.com/cankav/pytorcher\n",
    "* https://github.com/RyersonU-DataScienceLab/pomdp_solvers/blob/master/dist_utils/distributed_utils.py\n",
    "* https://pytorch.org/tutorials/intermediate/dist_tuto.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
